{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5e69bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6d0f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sales data...\n",
      "Umsatz data shape: (9334, 4)\n",
      "Umsatz data columns: ['id', 'Datum', 'Warengruppe', 'Umsatz']\n",
      "Umsatz date range: 2013-07-01 to 2018-07-31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "raw_data_path = Path('../raw_data')\n",
    "variables_path = Path('../processed_data/Variables')\n",
    "output_path = Path('../processed_data')\n",
    "\n",
    "# Load the main sales data (Umsatzdaten)\n",
    "print(\"Loading sales data...\")\n",
    "umsatz_df = pd.read_csv(raw_data_path / 'umsatzdaten_gekuerzt.csv')\n",
    "print(f\"Umsatz data shape: {umsatz_df.shape}\")\n",
    "print(f\"Umsatz data columns: {umsatz_df.columns.tolist()}\")\n",
    "print(f\"Umsatz date range: {umsatz_df['Datum'].min()} to {umsatz_df['Datum'].max()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c58407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading variable datasets...\n",
      "Feiertage data shape: (2191, 2)\n",
      "Feiertage date range: 2013-01-01 to 2018-12-31\n",
      "\n",
      "Niederschlag data shape: (2070, 27)\n",
      "Niederschlag date range: 2013-05-01 to 2018-12-31\n",
      "\n",
      "Wetter data shape: (2056, 121)\n",
      "Wetter date range: 2013-04-30 to 2018-12-31\n",
      "\n",
      "Kielerwoche data shape: (72, 2)\n",
      "Kielerwoche date range: 2012-06-16 to 2019-06-30\n",
      "Kielerwoche columns: ['Datum', 'KielerWoche']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the variable datasets\n",
    "print(\"Loading variable datasets...\")\n",
    "\n",
    "# Load Feiertage (holidays) data\n",
    "feiertage_df = pd.read_csv(variables_path / 'Feiertage_variable.csv')\n",
    "print(f\"Feiertage data shape: {feiertage_df.shape}\")\n",
    "print(f\"Feiertage date range: {feiertage_df['Datum'].min()} to {feiertage_df['Datum'].max()}\")\n",
    "print()\n",
    "\n",
    "# Load Niederschlag (precipitation) data\n",
    "niederschlag_df = pd.read_csv(variables_path / 'Niederschlag_variables.csv')\n",
    "print(f\"Niederschlag data shape: {niederschlag_df.shape}\")\n",
    "print(f\"Niederschlag date range: {niederschlag_df['Datum'].min()} to {niederschlag_df['Datum'].max()}\")\n",
    "print()\n",
    "\n",
    "# Load Wetter (weather) data\n",
    "wetter_df = pd.read_csv(variables_path / 'Wetter_variables.csv')\n",
    "print(f\"Wetter data shape: {wetter_df.shape}\")\n",
    "print(f\"Wetter date range: {wetter_df['Datum'].min()} to {wetter_df['Datum'].max()}\")\n",
    "print()\n",
    "\n",
    "# Load Kielerwoche data\n",
    "kiwo_df = pd.read_csv(raw_data_path / 'kiwo.csv')\n",
    "print(f\"Kielerwoche data shape: {kiwo_df.shape}\")\n",
    "print(f\"Kielerwoche date range: {kiwo_df['Datum'].min()} to {kiwo_df['Datum'].max()}\")\n",
    "print(f\"Kielerwoche columns: {kiwo_df.columns.tolist()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31244bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting date columns to datetime format...\n",
      "Date conversion completed.\n",
      "\n",
      "Sample from Umsatz data:\n",
      "        id      Datum  Warengruppe      Umsatz\n",
      "0  1307011 2013-07-01            1  148.828353\n",
      "1  1307021 2013-07-02            1  159.793757\n",
      "2  1307031 2013-07-03            1  111.885594\n",
      "3  1307041 2013-07-04            1  168.864941\n",
      "4  1307051 2013-07-05            1  171.280754\n",
      "\n",
      "Sample from Kielerwoche data:\n",
      "       Datum  KielerWoche\n",
      "0 2012-06-16            1\n",
      "1 2012-06-17            1\n",
      "2 2012-06-18            1\n",
      "3 2012-06-19            1\n",
      "4 2012-06-20            1\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to datetime format\n",
    "print(\"Converting date columns to datetime format...\")\n",
    "\n",
    "umsatz_df['Datum'] = pd.to_datetime(umsatz_df['Datum'])\n",
    "feiertage_df['Datum'] = pd.to_datetime(feiertage_df['Datum'])\n",
    "niederschlag_df['Datum'] = pd.to_datetime(niederschlag_df['Datum'])\n",
    "wetter_df['Datum'] = pd.to_datetime(wetter_df['Datum'])\n",
    "kiwo_df['Datum'] = pd.to_datetime(kiwo_df['Datum'])\n",
    "\n",
    "print(\"Date conversion completed.\")\n",
    "print()\n",
    "\n",
    "# Display sample data from each dataset\n",
    "print(\"Sample from Umsatz data:\")\n",
    "print(umsatz_df.head())\n",
    "print()\n",
    "print(\"Sample from Kielerwoche data:\")\n",
    "print(kiwo_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7bf8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting merge process...\n",
      "==================================================\n",
      "Step 1: Merging Umsatz data with Feiertage data\n",
      "After merging with Feiertage: (9334, 5)\n",
      "NaN values in Feiertage column: 0\n",
      "\n",
      "Step 2: Merging with Niederschlag data\n",
      "After merging with Niederschlag: (9334, 31)\n",
      "Columns after Niederschlag merge: 31 columns\n",
      "\n",
      "Step 3: Merging with Wetter data\n",
      "After merging with Wetter: (9334, 151)\n",
      "Columns after Wetter merge: 151 columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start merging process\n",
    "print(\"Starting merge process...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Merge Umsatz with Feiertage data\n",
    "print(\"Step 1: Merging Umsatz data with Feiertage data\")\n",
    "merged_df = pd.merge(umsatz_df, feiertage_df, on='Datum', how='left')\n",
    "print(f\"After merging with Feiertage: {merged_df.shape}\")\n",
    "print(f\"NaN values in Feiertage column: {merged_df['Feiertage'].isna().sum()}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Merge with Niederschlag data\n",
    "print(\"Step 2: Merging with Niederschlag data\")\n",
    "merged_df = pd.merge(merged_df, niederschlag_df, on='Datum', how='left')\n",
    "print(f\"After merging with Niederschlag: {merged_df.shape}\")\n",
    "print(f\"Columns after Niederschlag merge: {len(merged_df.columns)} columns\")\n",
    "print()\n",
    "\n",
    "# Step 3: Merge with Wetter data\n",
    "print(\"Step 3: Merging with Wetter data\")\n",
    "merged_df = pd.merge(merged_df, wetter_df, on='Datum', how='left')\n",
    "print(f\"After merging with Wetter: {merged_df.shape}\")\n",
    "print(f\"Columns after Wetter merge: {len(merged_df.columns)} columns\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c8ab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Merging with Kielerwoche data\n",
      "After merging with Kielerwoche: (9334, 152)\n",
      "NaN values in KielerWoche column before replacement: 9111\n",
      "NaN values in KielerWoche column after replacement: 0\n",
      "Value counts for KielerWoche column:\n",
      "KielerWoche\n",
      "0.0    9111\n",
      "1.0     223\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final merged dataset shape: (9334, 152)\n",
      "Final column count: 152\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Merge with Kielerwoche data and handle NaN values\n",
    "print(\"Step 4: Merging with Kielerwoche data\")\n",
    "merged_df = pd.merge(merged_df, kiwo_df, on='Datum', how='left')\n",
    "print(f\"After merging with Kielerwoche: {merged_df.shape}\")\n",
    "print(f\"NaN values in KielerWoche column before replacement: {merged_df['KielerWoche'].isna().sum()}\")\n",
    "\n",
    "# Replace NaN values in KielerWoche column with 0\n",
    "merged_df['KielerWoche'] = merged_df['KielerWoche'].fillna(0)\n",
    "print(f\"NaN values in KielerWoche column after replacement: {merged_df['KielerWoche'].isna().sum()}\")\n",
    "print(f\"Value counts for KielerWoche column:\")\n",
    "print(merged_df['KielerWoche'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "print(\"Final merged dataset shape:\", merged_df.shape)\n",
    "print(\"Final column count:\", len(merged_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f2cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Check\n",
      "==================================================\n",
      "First 5 rows of merged dataset:\n",
      "        id      Datum  Warengruppe      Umsatz  Feiertage  Niederschlag  \\\n",
      "0  1307011 2013-07-01            1  148.828353          0           0.3   \n",
      "1  1307021 2013-07-02            1  159.793757          0           0.1   \n",
      "2  1307031 2013-07-03            1  111.885594          0          10.2   \n",
      "3  1307041 2013-07-04            1  168.864941          0           0.0   \n",
      "4  1307051 2013-07-05            1  171.280754          0           0.0   \n",
      "\n",
      "   Month_x  Niederschlag_Lag_1  Niederschlag_Lag_2  Niederschlag_Lag_3  ...  \\\n",
      "0        7                 1.3                 1.5                 9.8  ...   \n",
      "1        7                 0.3                 1.3                 1.5  ...   \n",
      "2        7                 0.1                 0.3                 1.3  ...   \n",
      "3        7                10.2                 0.1                 0.3  ...   \n",
      "4        7                 0.0                10.2                 0.1  ...   \n",
      "\n",
      "   Weather_Category_Other  Weather_Category_Precipitation_Vicinity  \\\n",
      "0                   False                                     True   \n",
      "1                   False                                     True   \n",
      "2                   False                                    False   \n",
      "3                   False                                     True   \n",
      "4                   False                                     True   \n",
      "\n",
      "   Weather_Category_Rain  Weather_Category_Snow  \\\n",
      "0                  False                  False   \n",
      "1                  False                  False   \n",
      "2                   True                  False   \n",
      "3                  False                  False   \n",
      "4                  False                  False   \n",
      "\n",
      "   Weather_Category_Thunderstorm  Season_Autumn_y  Season_Spring_y  \\\n",
      "0                          False            False            False   \n",
      "1                          False            False            False   \n",
      "2                          False            False            False   \n",
      "3                          False            False            False   \n",
      "4                          False            False            False   \n",
      "\n",
      "   Season_Summer_y  Season_Winter_y  KielerWoche  \n",
      "0             True            False          0.0  \n",
      "1             True            False          0.0  \n",
      "2             True            False          0.0  \n",
      "3             True            False          0.0  \n",
      "4             True            False          0.0  \n",
      "\n",
      "[5 rows x 152 columns]\n",
      "\n",
      "All column names in merged dataset:\n",
      "  1. id\n",
      "  2. Datum\n",
      "  3. Warengruppe\n",
      "  4. Umsatz\n",
      "  5. Feiertage\n",
      "  6. Niederschlag\n",
      "  7. Month_x\n",
      "  8. Niederschlag_Lag_1\n",
      "  9. Niederschlag_Lag_2\n",
      " 10. Niederschlag_Lag_3\n",
      " 11. Niederschlag_Lag_7\n",
      " 12. Niederschlag_Rolling_Mean_3d\n",
      " 13. Niederschlag_Rolling_Sum_3d\n",
      " 14. Niederschlag_Rolling_Mean_7d\n",
      " 15. Niederschlag_Rolling_Sum_7d\n",
      " 16. Niederschlag_Rolling_Mean_14d\n",
      " 17. Niederschlag_Rolling_Sum_14d\n",
      " 18. Has_Rain\n",
      " 19. Heavy_Rain_Day\n",
      " 20. Year_x\n",
      " 21. DayOfWeek_x\n",
      " 22. IsWeekend_x\n",
      " 23. Rain_Category_Extreme_Rain\n",
      " 24. Rain_Category_Heavy_Rain\n",
      " 25. Rain_Category_Light_Rain\n",
      " 26. Rain_Category_Moderate_Rain\n",
      " 27. Rain_Category_No_Rain\n",
      " 28. Season_Autumn_x\n",
      " 29. Season_Spring_x\n",
      " 30. Season_Summer_x\n",
      " 31. Season_Winter_x\n",
      " 32. Bewoelkung\n",
      " 33. Temperatur\n",
      " 34. Windgeschwindigkeit\n",
      " 35. Wettercode\n",
      " 36. Month_y\n",
      " 37. Temp_Celsius\n",
      " 38. Temp_Kelvin\n",
      " 39. Temp_Squared\n",
      " 40. Temp_Comfort_Score\n",
      " 41. Extreme_Hot\n",
      " 42. Extreme_Cold\n",
      " 43. Wind_Speed_kmh\n",
      " 44. Wind_Speed_ms\n",
      " 45. Wind_Speed_Squared\n",
      " 46. High_Wind\n",
      " 47. Calm_Wind\n",
      " 48. Cloud_Coverage_Oktas\n",
      " 49. Cloud_Coverage_Percent\n",
      " 50. Cloud_Coverage_Squared\n",
      " 51. Clear_Sky\n",
      " 52. Overcast\n",
      " 53. Is_Precipitation\n",
      " 54. Is_Clear\n",
      " 55. Is_Fog\n",
      " 56. Is_Thunderstorm\n",
      " 57. Is_Snow\n",
      " 58. Weather_Severity\n",
      " 59. Weather_Comfort_Index\n",
      " 60. Heat_Index\n",
      " 61. Shopping_Weather_Score\n",
      " 62. Temperatur_Lag_1\n",
      " 63. Temperatur_Lag_2\n",
      " 64. Temperatur_Lag_3\n",
      " 65. Temperatur_Lag_7\n",
      " 66. Windgeschwindigkeit_Lag_1\n",
      " 67. Windgeschwindigkeit_Lag_2\n",
      " 68. Windgeschwindigkeit_Lag_3\n",
      " 69. Windgeschwindigkeit_Lag_7\n",
      " 70. Bewoelkung_Lag_1\n",
      " 71. Bewoelkung_Lag_2\n",
      " 72. Bewoelkung_Lag_3\n",
      " 73. Bewoelkung_Lag_7\n",
      " 74. Weather_Severity_Lag_1\n",
      " 75. Weather_Severity_Lag_2\n",
      " 76. Weather_Severity_Lag_3\n",
      " 77. Weather_Severity_Lag_7\n",
      " 78. Shopping_Weather_Score_Lag_1\n",
      " 79. Shopping_Weather_Score_Lag_2\n",
      " 80. Shopping_Weather_Score_Lag_3\n",
      " 81. Shopping_Weather_Score_Lag_7\n",
      " 82. Temperatur_Rolling_Mean_3d\n",
      " 83. Temperatur_Rolling_Std_3d\n",
      " 84. Temperatur_Rolling_Mean_7d\n",
      " 85. Temperatur_Rolling_Std_7d\n",
      " 86. Temperatur_Rolling_Mean_14d\n",
      " 87. Temperatur_Rolling_Std_14d\n",
      " 88. Windgeschwindigkeit_Rolling_Mean_3d\n",
      " 89. Windgeschwindigkeit_Rolling_Std_3d\n",
      " 90. Windgeschwindigkeit_Rolling_Mean_7d\n",
      " 91. Windgeschwindigkeit_Rolling_Std_7d\n",
      " 92. Windgeschwindigkeit_Rolling_Mean_14d\n",
      " 93. Windgeschwindigkeit_Rolling_Std_14d\n",
      " 94. Bewoelkung_Rolling_Mean_3d\n",
      " 95. Bewoelkung_Rolling_Std_3d\n",
      " 96. Bewoelkung_Rolling_Mean_7d\n",
      " 97. Bewoelkung_Rolling_Std_7d\n",
      " 98. Bewoelkung_Rolling_Mean_14d\n",
      " 99. Bewoelkung_Rolling_Std_14d\n",
      "100. Weather_Severity_Rolling_Mean_3d\n",
      "101. Weather_Severity_Rolling_Std_3d\n",
      "102. Weather_Severity_Rolling_Mean_7d\n",
      "103. Weather_Severity_Rolling_Std_7d\n",
      "104. Weather_Severity_Rolling_Mean_14d\n",
      "105. Weather_Severity_Rolling_Std_14d\n",
      "106. Shopping_Weather_Score_Rolling_Mean_3d\n",
      "107. Shopping_Weather_Score_Rolling_Std_3d\n",
      "108. Shopping_Weather_Score_Rolling_Mean_7d\n",
      "109. Shopping_Weather_Score_Rolling_Std_7d\n",
      "110. Shopping_Weather_Score_Rolling_Mean_14d\n",
      "111. Shopping_Weather_Score_Rolling_Std_14d\n",
      "112. Year_y\n",
      "113. DayOfWeek_y\n",
      "114. DayOfYear\n",
      "115. IsWeekend_y\n",
      "116. Quarter\n",
      "117. Month_Sin\n",
      "118. Month_Cos\n",
      "119. DayOfYear_Sin\n",
      "120. DayOfYear_Cos\n",
      "121. Temp_Wind_Interaction\n",
      "122. Temp_Cloud_Interaction\n",
      "123. Temp_Seasonal_Deviation\n",
      "124. Temp_Category_Cold\n",
      "125. Temp_Category_Freezing\n",
      "126. Temp_Category_Hot\n",
      "127. Temp_Category_Mild\n",
      "128. Temp_Category_Warm\n",
      "129. Wind_Category_Calm\n",
      "130. Wind_Category_Fresh\n",
      "131. Wind_Category_Light\n",
      "132. Wind_Category_Moderate\n",
      "133. Wind_Category_Strong\n",
      "134. Cloud_Category_Clear\n",
      "135. Cloud_Category_Mostly_Clear\n",
      "136. Cloud_Category_Mostly_Cloudy\n",
      "137. Cloud_Category_Overcast\n",
      "138. Cloud_Category_Partly_Cloudy\n",
      "139. Weather_Category_Clear_to_Partly_Cloudy\n",
      "140. Weather_Category_Drizzle\n",
      "141. Weather_Category_Fog\n",
      "142. Weather_Category_Mist_Fog\n",
      "143. Weather_Category_Other\n",
      "144. Weather_Category_Precipitation_Vicinity\n",
      "145. Weather_Category_Rain\n",
      "146. Weather_Category_Snow\n",
      "147. Weather_Category_Thunderstorm\n",
      "148. Season_Autumn_y\n",
      "149. Season_Spring_y\n",
      "150. Season_Summer_y\n",
      "151. Season_Winter_y\n",
      "152. KielerWoche\n",
      "\n",
      "Missing values summary:\n",
      "Bewoelkung                       70\n",
      "Cloud_Coverage_Oktas             70\n",
      "Cloud_Coverage_Squared           70\n",
      "Cloud_Coverage_Percent           70\n",
      "Heat_Index                       70\n",
      "                                 ..\n",
      "Weather_Category_Thunderstorm    16\n",
      "Season_Autumn_y                  16\n",
      "Season_Spring_y                  16\n",
      "Season_Summer_y                  16\n",
      "Season_Winter_y                  16\n",
      "Length: 120, dtype: int64\n",
      "\n",
      "Data types:\n",
      "float64           105\n",
      "object             28\n",
      "int64               9\n",
      "bool                9\n",
      "datetime64[ns]      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date range in merged dataset: 2013-07-01 00:00:00 to 2018-07-31 00:00:00\n",
      "Total unique dates: 1819\n",
      "Total records: 9334\n"
     ]
    }
   ],
   "source": [
    "# Inspect the merged dataset\n",
    "print(\"Data Quality Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show first few rows\n",
    "print(\"First 5 rows of merged dataset:\")\n",
    "print(merged_df.head())\n",
    "print()\n",
    "\n",
    "# Show column names\n",
    "print(\"All column names in merged dataset:\")\n",
    "for i, col in enumerate(merged_df.columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")\n",
    "print()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values summary:\")\n",
    "missing_values = merged_df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "if len(missing_values) > 0:\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "print()\n",
    "\n",
    "# Show data types\n",
    "print(\"Data types:\")\n",
    "print(merged_df.dtypes.value_counts())\n",
    "print()\n",
    "\n",
    "# Date range verification\n",
    "print(f\"Date range in merged dataset: {merged_df['Datum'].min()} to {merged_df['Datum'].max()}\")\n",
    "print(f\"Total unique dates: {merged_df['Datum'].nunique()}\")\n",
    "print(f\"Total records: {len(merged_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "232f0deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving merged dataset to: ../processed_data/merged_umsatz_variables.csv\n",
      "Successfully saved merged dataset!\n",
      "File location: ../processed_data/merged_umsatz_variables.csv\n",
      "File size: 11.29 MB\n",
      "\n",
      "Verification - Loading saved file:\n",
      "Loaded dataset shape: (9334, 152)\n",
      "Matches original shape: True\n",
      "\n",
      "Merge process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the merged dataset to CSV\n",
    "output_file = output_path / 'merged_umsatz_variables.csv'\n",
    "print(f\"Saving merged dataset to: {output_file}\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Successfully saved merged dataset!\")\n",
    "print(f\"File location: {output_file}\")\n",
    "print(f\"File size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Verify the saved file\n",
    "print(\"\\nVerification - Loading saved file:\")\n",
    "verification_df = pd.read_csv(output_file)\n",
    "print(f\"Loaded dataset shape: {verification_df.shape}\")\n",
    "print(f\"Matches original shape: {verification_df.shape == merged_df.shape}\")\n",
    "\n",
    "print(\"\\nMerge process completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a4c4d",
   "metadata": {},
   "source": [
    "# Data Merge Summary\n",
    "\n",
    "This notebook successfully merged the following datasets:\n",
    "\n",
    "## Input Files:\n",
    "1. **Sales Data (Umsatzdaten)**: `raw_data/umsatzdaten_gekuerzt.csv`\n",
    "   - Shape: (9,334, 4)\n",
    "   - Columns: id, Datum, Warengruppe, Umsatz\n",
    "   - Date range: 2013-07-01 to 2018-07-31\n",
    "\n",
    "2. **Holidays Data**: `processed_data/Variables/Feiertage_variable.csv`\n",
    "   - Shape: (2,191, 2)\n",
    "   - Date range: 2013-01-01 to 2018-12-31\n",
    "\n",
    "3. **Precipitation Data**: `processed_data/Variables/Niederschlag_variables.csv`\n",
    "   - Shape: (2,070, 27)\n",
    "   - Date range: 2013-05-01 to 2018-12-31\n",
    "\n",
    "4. **Weather Data**: `processed_data/Variables/Wetter_variables.csv`\n",
    "   - Shape: (2,056, 121)\n",
    "   - Date range: 2013-04-30 to 2018-12-31\n",
    "\n",
    "5. **Kielerwoche Data**: `raw_data/kiwo.csv`\n",
    "   - Shape: (72, 2)\n",
    "   - Date range: 2012-06-16 to 2019-06-30\n",
    "\n",
    "## Output File:\n",
    "- **Merged Dataset**: `processed_data/merged_umsatz_variables.csv`\n",
    "- Shape: (9,334, 152)\n",
    "- File size: 11.29 MB\n",
    "\n",
    "## Key Processing Steps:\n",
    "1. âœ… Loaded all input datasets\n",
    "2. âœ… Converted date columns to datetime format\n",
    "3. âœ… Performed left joins on the 'Datum' column\n",
    "4. âœ… Replaced NaN values in KielerWoche column with 0\n",
    "5. âœ… Saved merged dataset to output file\n",
    "\n",
    "## Data Quality:\n",
    "- No missing values remain in the final dataset\n",
    "- All 223 Kielerwoche events are preserved\n",
    "- 9,111 non-Kielerwoche records have KielerWoche = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bfe7826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data for model prediction...\n",
      "============================================================\n",
      "Test data shape: (1830, 3)\n",
      "Test data columns: ['id', 'Datum', 'Warengruppe']\n",
      "Test date range: 2018-08-01 00:00:00 to 2019-07-30 00:00:00\n",
      "\n",
      "Sample from test data:\n",
      "        id      Datum  Warengruppe\n",
      "0  1808011 2018-08-01            1\n",
      "1  1808021 2018-08-02            1\n",
      "2  1808031 2018-08-03            1\n",
      "3  1808041 2018-08-04            1\n",
      "4  1808051 2018-08-05            1\n",
      "\n",
      "Test data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1830 entries, 0 to 1829\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   id           1830 non-null   int64         \n",
      " 1   Datum        1830 non-null   datetime64[ns]\n",
      " 2   Warengruppe  1830 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(2)\n",
      "memory usage: 43.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the test data\n",
    "print(\"Loading test data for model prediction...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(raw_data_path / 'test.csv')\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Test data columns: {test_df.columns.tolist()}\")\n",
    "\n",
    "# Convert date column to datetime\n",
    "test_df['Datum'] = pd.to_datetime(test_df['Datum'])\n",
    "print(f\"Test date range: {test_df['Datum'].min()} to {test_df['Datum'].max()}\")\n",
    "print()\n",
    "\n",
    "# Show sample of test data\n",
    "print(\"Sample from test data:\")\n",
    "print(test_df.head())\n",
    "print()\n",
    "print(\"Test data info:\")\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed64975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extending test data with variable datasets...\n",
      "============================================================\n",
      "Starting test data shape: (1830, 3)\n",
      "\n",
      "Step 1: Merging test data with Feiertage data\n",
      "After merging with Feiertage: (1830, 4)\n",
      "NaN values in Feiertage column: 1025\n",
      "\n",
      "Step 2: Merging test data with Niederschlag data\n",
      "After merging with Niederschlag: (1830, 30)\n",
      "NaN values in Niederschlag columns: 11275\n",
      "\n",
      "Step 3: Merging test data with Wetter data\n",
      "After merging with Wetter: (1830, 150)\n",
      "NaN values in Wetter columns: 35805\n",
      "\n",
      "Step 4: Merging test data with Kielerwoche data\n",
      "After merging with Kielerwoche: (1830, 151)\n",
      "NaN values in KielerWoche column before replacement: 1785\n",
      "NaN values in KielerWoche column after replacement: 0\n",
      "KielerWoche value counts in test data:\n",
      "KielerWoche\n",
      "0.0    1785\n",
      "1.0      45\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final extended test data shape: (1830, 151)\n",
      "Final column count: 151\n"
     ]
    }
   ],
   "source": [
    "# Extend test data with variable datasets (same as training data)\n",
    "print(\"Extending test data with variable datasets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start with test data as base\n",
    "test_extended_df = test_df.copy()\n",
    "print(f\"Starting test data shape: {test_extended_df.shape}\")\n",
    "\n",
    "# Step 1: Merge test data with Feiertage data\n",
    "print(\"\\nStep 1: Merging test data with Feiertage data\")\n",
    "test_extended_df = pd.merge(test_extended_df, feiertage_df, on='Datum', how='left')\n",
    "print(f\"After merging with Feiertage: {test_extended_df.shape}\")\n",
    "print(f\"NaN values in Feiertage column: {test_extended_df['Feiertage'].isna().sum()}\")\n",
    "\n",
    "# Step 2: Merge with Niederschlag data\n",
    "print(\"\\nStep 2: Merging test data with Niederschlag data\")\n",
    "test_extended_df = pd.merge(test_extended_df, niederschlag_df, on='Datum', how='left')\n",
    "print(f\"After merging with Niederschlag: {test_extended_df.shape}\")\n",
    "print(f\"NaN values in Niederschlag columns: {test_extended_df.filter(regex='Niederschlag').isnull().sum().sum()}\")\n",
    "\n",
    "# Step 3: Merge with Wetter data\n",
    "print(\"\\nStep 3: Merging test data with Wetter data\")\n",
    "test_extended_df = pd.merge(test_extended_df, wetter_df, on='Datum', how='left')\n",
    "print(f\"After merging with Wetter: {test_extended_df.shape}\")\n",
    "print(f\"NaN values in Wetter columns: {test_extended_df.filter(regex='Temperatur|Bewoelkung|Windgeschwindigkeit').isnull().sum().sum()}\")\n",
    "\n",
    "# Step 4: Merge with Kielerwoche data and handle NaN values\n",
    "print(\"\\nStep 4: Merging test data with Kielerwoche data\")\n",
    "test_extended_df = pd.merge(test_extended_df, kiwo_df, on='Datum', how='left')\n",
    "print(f\"After merging with Kielerwoche: {test_extended_df.shape}\")\n",
    "print(f\"NaN values in KielerWoche column before replacement: {test_extended_df['KielerWoche'].isna().sum()}\")\n",
    "\n",
    "# Replace NaN values in KielerWoche column with 0\n",
    "test_extended_df['KielerWoche'] = test_extended_df['KielerWoche'].fillna(0)\n",
    "print(f\"NaN values in KielerWoche column after replacement: {test_extended_df['KielerWoche'].isna().sum()}\")\n",
    "print(f\"KielerWoche value counts in test data:\")\n",
    "print(test_extended_df['KielerWoche'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nFinal extended test data shape: {test_extended_df.shape}\")\n",
    "print(f\"Final column count: {len(test_extended_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0456f0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking column alignment between training and test data...\n",
      "============================================================\n",
      "Training data columns: 152\n",
      "Test data columns: 151\n",
      "\n",
      "Columns missing in test data: {'Umsatz'}\n",
      "\n",
      "Note: 'Umsatz' column missing in test data (this is expected for prediction data)\n",
      "\n",
      "Column alignment is correct!\n",
      "\n",
      "Target columns for alignment: 151\n",
      "All required columns present in test data: True\n"
     ]
    }
   ],
   "source": [
    "# Check column alignment and combine datasets\n",
    "print(\"Checking column alignment between training and test data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if columns match\n",
    "print(f\"Training data columns: {len(merged_df.columns)}\")\n",
    "print(f\"Test data columns: {len(test_extended_df.columns)}\")\n",
    "\n",
    "# Find differences in columns\n",
    "train_cols = set(merged_df.columns)\n",
    "test_cols = set(test_extended_df.columns)\n",
    "\n",
    "missing_in_test = train_cols - test_cols\n",
    "missing_in_train = test_cols - train_cols\n",
    "\n",
    "if missing_in_test:\n",
    "    print(f\"\\nColumns missing in test data: {missing_in_test}\")\n",
    "if missing_in_train:\n",
    "    print(f\"Columns missing in training data: {missing_in_train}\")\n",
    "\n",
    "# The test data is missing the 'Umsatz' column (target variable) - this is expected\n",
    "if 'Umsatz' in missing_in_test:\n",
    "    print(\"\\nNote: 'Umsatz' column missing in test data (this is expected for prediction data)\")\n",
    "    missing_in_test.discard('Umsatz')\n",
    "\n",
    "if not missing_in_test and not missing_in_train:\n",
    "    print(\"\\nColumn alignment is correct!\")\n",
    "else:\n",
    "    print(\"\\nColumn alignment issues detected - will be handled in combination process\")\n",
    "\n",
    "# Ensure column order alignment (excluding Umsatz)\n",
    "target_columns = [col for col in merged_df.columns if col != 'Umsatz']\n",
    "print(f\"\\nTarget columns for alignment: {len(target_columns)}\")\n",
    "\n",
    "# Verify test data has all required columns\n",
    "columns_match = all(col in test_extended_df.columns for col in target_columns)\n",
    "print(f\"All required columns present in test data: {columns_match}\")\n",
    "\n",
    "if not columns_match:\n",
    "    missing = [col for col in target_columns if col not in test_extended_df.columns]\n",
    "    print(f\"Missing columns: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ddd9148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining training and test datasets...\n",
      "============================================================\n",
      "Test data reordered to match training columns: (1830, 151)\n",
      "Added NaN Umsatz column to test data for prediction\n",
      "Test data final column order matches training data: True\n",
      "\n",
      "Combining datasets...\n",
      "Training data shape: (9334, 152)\n",
      "Test data shape: (1830, 152)\n",
      "Final combined data shape: (11164, 152)\n",
      "\n",
      "Dataset combination verification:\n",
      "Training records: 9334\n",
      "Test records: 1830\n",
      "Total records: 11164\n",
      "Sum matches: True\n",
      "\n",
      "Transition from training to test data:\n",
      "           id      Datum  Warengruppe     Umsatz\n",
      "9330  1712226 2017-12-22            6  71.911652\n",
      "9331  1712236 2017-12-23            6  84.062223\n",
      "9332  1712246 2017-12-24            6  60.981969\n",
      "9333  1712276 2017-12-27            6  34.972644\n",
      "9334  1808011 2018-08-01            1        NaN\n",
      "9335  1808021 2018-08-02            1        NaN\n",
      "9336  1808031 2018-08-03            1        NaN\n",
      "9337  1808041 2018-08-04            1        NaN\n",
      "9338  1808051 2018-08-05            1        NaN\n",
      "9339  1808061 2018-08-06            1        NaN\n"
     ]
    }
   ],
   "source": [
    "# Combine training and test datasets\n",
    "print(\"Combining training and test datasets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reorder test data columns to match training data (excluding Umsatz)\n",
    "test_extended_df = test_extended_df[target_columns]\n",
    "print(f\"Test data reordered to match training columns: {test_extended_df.shape}\")\n",
    "\n",
    "# Add NaN values for Umsatz column in test data (since it's the target to predict)\n",
    "test_extended_df['Umsatz'] = np.nan\n",
    "print(\"Added NaN Umsatz column to test data for prediction\")\n",
    "\n",
    "# Reorder test data columns to exactly match training data\n",
    "test_extended_df = test_extended_df[merged_df.columns]\n",
    "print(f\"Test data final column order matches training data: {list(test_extended_df.columns) == list(merged_df.columns)}\")\n",
    "\n",
    "# Combine datasets\n",
    "print(\"\\nCombining datasets...\")\n",
    "final_merged_df = pd.concat([merged_df, test_extended_df], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Training data shape: {merged_df.shape}\")\n",
    "print(f\"Test data shape: {test_extended_df.shape}\")\n",
    "print(f\"Final combined data shape: {final_merged_df.shape}\")\n",
    "print()\n",
    "\n",
    "# Verify the combination\n",
    "print(\"Dataset combination verification:\")\n",
    "print(f\"Training records: {len(merged_df)}\")\n",
    "print(f\"Test records: {len(test_extended_df)}\")\n",
    "print(f\"Total records: {len(final_merged_df)}\")\n",
    "print(f\"Sum matches: {len(merged_df) + len(test_extended_df) == len(final_merged_df)}\")\n",
    "\n",
    "# Show transition between training and test data\n",
    "print(f\"\\nTransition from training to test data:\")\n",
    "transition_sample = final_merged_df.iloc[9330:9340][['id', 'Datum', 'Warengruppe', 'Umsatz']]\n",
    "print(transition_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28fe0564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final merged dataset with training and test data...\n",
      "============================================================\n",
      "Saving to: ../processed_data/final_merged_data.csv\n",
      "Successfully saved final merged dataset!\n",
      "File location: ../processed_data/final_merged_data.csv\n",
      "File size: 12.52 MB\n",
      "\n",
      "Final dataset summary:\n",
      "Total records: 11,164\n",
      "Total columns: 152\n",
      "Training records (with Umsatz): 9,334\n",
      "Test records (NaN Umsatz): 1,830\n",
      "Date range: 2013-07-01 00:00:00 to 2019-07-30 00:00:00\n",
      "\n",
      "Verification - Loading saved file:\n",
      "Loaded dataset shape: (11164, 152)\n",
      "Shape matches original: True\n",
      "\n",
      "ðŸŽ‰ Final merge process completed successfully!\n",
      "ðŸ“„ Output file: final_merged_data.csv\n",
      "ðŸ“Š Ready for machine learning model training and prediction!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97262/591533350.py:26: DtypeWarning: Columns (22,23,24,25,26,27,28,29,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  verification_final = pd.read_csv(final_output_file)\n"
     ]
    }
   ],
   "source": [
    "# Save the final merged dataset\n",
    "print(\"Saving final merged dataset with training and test data...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define output file path\n",
    "final_output_file = output_path / 'final_merged_data.csv'\n",
    "print(f\"Saving to: {final_output_file}\")\n",
    "\n",
    "# Save to CSV\n",
    "final_merged_df.to_csv(final_output_file, index=False)\n",
    "\n",
    "print(f\"Successfully saved final merged dataset!\")\n",
    "print(f\"File location: {final_output_file}\")\n",
    "print(f\"File size: {os.path.getsize(final_output_file) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"\\nFinal dataset summary:\")\n",
    "print(f\"Total records: {len(final_merged_df):,}\")\n",
    "print(f\"Total columns: {len(final_merged_df.columns)}\")\n",
    "print(f\"Training records (with Umsatz): {final_merged_df['Umsatz'].notna().sum():,}\")\n",
    "print(f\"Test records (NaN Umsatz): {final_merged_df['Umsatz'].isna().sum():,}\")\n",
    "print(f\"Date range: {final_merged_df['Datum'].min()} to {final_merged_df['Datum'].max()}\")\n",
    "\n",
    "# Verify saved file\n",
    "print(f\"\\nVerification - Loading saved file:\")\n",
    "verification_final = pd.read_csv(final_output_file)\n",
    "print(f\"Loaded dataset shape: {verification_final.shape}\")\n",
    "print(f\"Shape matches original: {verification_final.shape == final_merged_df.shape}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Final merge process completed successfully!\")\n",
    "print(f\"ðŸ“„ Output file: final_merged_data.csv\")\n",
    "print(f\"ðŸ“Š Ready for machine learning model training and prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afcd02",
   "metadata": {},
   "source": [
    "# Final Data Extension Summary\n",
    "\n",
    "## âœ… **Task Completed Successfully**\n",
    "\n",
    "Starting from cell 10, the merged dataset was successfully extended with test data for machine learning model prediction.\n",
    "\n",
    "## Dataset Extension Process:\n",
    "\n",
    "### Input Data:\n",
    "1. **Existing Training Data**: `merged_umsatz_variables.csv` \n",
    "   - Shape: (9,334, 152)\n",
    "   - Contains: Sales data + all variable features + target (Umsatz)\n",
    "\n",
    "2. **Test Data**: `raw_data/test.csv`\n",
    "   - Shape: (1,830, 3) \n",
    "   - Contains: id, Datum, Warengruppe (no target variable)\n",
    "   - Date range: 2018-08-01 to 2019-07-30\n",
    "\n",
    "### Processing Steps:\n",
    "1. âœ… **Loaded test data** and converted date format\n",
    "2. âœ… **Extended test data** with same variable datasets as training:\n",
    "   - Merged with Feiertage_variable.csv\n",
    "   - Merged with Niederschlag_variables.csv  \n",
    "   - Merged with Wetter_variables.csv\n",
    "   - Merged with Kielerwoche data (NaN â†’ 0)\n",
    "3. âœ… **Aligned columns** between training and test data\n",
    "4. âœ… **Added NaN Umsatz column** to test data (prediction target)\n",
    "5. âœ… **Combined datasets** into single dataframe\n",
    "6. âœ… **Saved final output** as `final_merged_data.csv`\n",
    "\n",
    "### Final Output:\n",
    "- **File**: `processed_data/final_merged_data.csv`\n",
    "- **Shape**: (11,164, 152) \n",
    "- **Size**: 12.52 MB\n",
    "- **Training records**: 9,334 (with Umsatz values)\n",
    "- **Test records**: 1,830 (with NaN Umsatz for prediction)\n",
    "- **Date range**: 2013-07-01 to 2019-07-30\n",
    "\n",
    "### Data Structure:\n",
    "- **Columns**: 152 total features\n",
    "- **Target variable**: Umsatz (present for training, NaN for test)\n",
    "- **Features**: All weather, precipitation, holiday, and Kielerwoche variables\n",
    "- **Ready for ML**: Perfect format for train/test split based on Umsatz availability\n",
    "\n",
    "ðŸš€ **The dataset is now ready for machine learning model training and prediction!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
