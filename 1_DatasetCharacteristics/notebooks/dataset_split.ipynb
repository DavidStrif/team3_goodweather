{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c12473",
   "metadata": {},
   "source": [
    "## Load the Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset loaded successfully\n",
      "Shape: (11782, 11)\n",
      "Date range: 2012-01-01 00:00:00 to 2019-12-31 00:00:00\n",
      "\n",
      "First few rows:\n",
      "       Datum  id  Warengruppe  Umsatz  KielerWoche  Bewoelkung  Temperatur  \\\n",
      "0 2012-01-01 NaN          NaN     NaN          NaN         8.0      9.8250   \n",
      "1 2012-01-02 NaN          NaN     NaN          NaN         7.0      7.4375   \n",
      "2 2012-01-03 NaN          NaN     NaN          NaN         8.0      5.5375   \n",
      "3 2012-01-04 NaN          NaN     NaN          NaN         4.0      5.6875   \n",
      "4 2012-01-05 NaN          NaN     NaN          NaN         6.0      5.3000   \n",
      "\n",
      "   Windgeschwindigkeit  Wettercode  Niederschlag  is_holiday  \n",
      "0                 14.0        58.0          14.0           0  \n",
      "1                 12.0         NaN           0.0           0  \n",
      "2                 18.0        63.0          20.8           0  \n",
      "3                 19.0        80.0          19.7           0  \n",
      "4                 23.0        80.0           3.3           0  \n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11782 entries, 0 to 11781\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   Datum                11782 non-null  datetime64[ns]\n",
      " 1   id                   11164 non-null  float64       \n",
      " 2   Warengruppe          11164 non-null  float64       \n",
      " 3   Umsatz               9334 non-null   float64       \n",
      " 4   KielerWoche          286 non-null    float64       \n",
      " 5   Bewoelkung           11471 non-null  float64       \n",
      " 6   Temperatur           11526 non-null  float64       \n",
      " 7   Windgeschwindigkeit  11526 non-null  float64       \n",
      " 8   Wettercode           8785 non-null   float64       \n",
      " 9   Niederschlag         11779 non-null  float64       \n",
      " 10  is_holiday           11782 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1)\n",
      "memory usage: 1012.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set up path for processed data\n",
    "processed_data_dir = '../processed_data'\n",
    "\n",
    "# Load the combined dataset\n",
    "file_path = os.path.join(processed_data_dir, 'combined_data_final.csv')\n",
    "df = pd.read_csv(file_path, parse_dates=['Datum'])\n",
    "\n",
    "print(f\"Combined dataset loaded successfully\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Datum'].min()} to {df['Datum'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cf7a3",
   "metadata": {},
   "source": [
    "## Define Date Ranges and Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba574891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits created successfully!\n",
      "\n",
      "Training set: 2013-07-01 to 2017-07-31\n",
      "  Rows: 7523\n",
      "\n",
      "Validation set: 2017-08-01 to 2018-07-31\n",
      "  Rows: 1849\n",
      "\n",
      "Test set: 2018-08-01 to 2019-07-31\n",
      "  Rows: 1840\n",
      "\n",
      "Total rows in all splits: 11212\n"
     ]
    }
   ],
   "source": [
    "# Define the date ranges for the splits\n",
    "train_start = pd.Timestamp('2013-07-01')\n",
    "train_end = pd.Timestamp('2017-07-31')\n",
    "\n",
    "val_start = pd.Timestamp('2017-08-01')\n",
    "val_end = pd.Timestamp('2018-07-31')\n",
    "\n",
    "test_start = pd.Timestamp('2018-08-01')\n",
    "test_end = pd.Timestamp('2019-07-31')\n",
    "\n",
    "# Split the dataset based on date ranges\n",
    "df_train = df[(df['Datum'] >= train_start) & (df['Datum'] <= train_end)].copy()\n",
    "df_validation = df[(df['Datum'] >= val_start) & (df['Datum'] <= val_end)].copy()\n",
    "df_test = df[(df['Datum'] >= test_start) & (df['Datum'] <= test_end)].copy()\n",
    "\n",
    "print(\"Dataset splits created successfully!\\n\")\n",
    "print(f\"Training set: {train_start.date()} to {train_end.date()}\")\n",
    "print(f\"  Rows: {len(df_train)}\")\n",
    "print(f\"\\nValidation set: {val_start.date()} to {val_end.date()}\")\n",
    "print(f\"  Rows: {len(df_validation)}\")\n",
    "print(f\"\\nTest set: {test_start.date()} to {test_end.date()}\")\n",
    "print(f\"  Rows: {len(df_test)}\")\n",
    "print(f\"\\nTotal rows in all splits: {len(df_train) + len(df_validation) + len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba1cba",
   "metadata": {},
   "source": [
    "## Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a4d722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA INTEGRITY VERIFICATION\n",
      "============================================================\n",
      "\n",
      "1. Missing Values Analysis:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Training Set:\n",
      "  Total rows: 7523\n",
      "  Rows with id: 7493\n",
      "  Rows with Umsatz: 7493\n",
      "  Rows with both id and Umsatz: 7493\n",
      "\n",
      "Validation Set:\n",
      "  Total rows: 1849\n",
      "  Rows with id: 1841\n",
      "  Rows with Umsatz: 1841\n",
      "  Rows with both id and Umsatz: 1841\n",
      "\n",
      "Test Set:\n",
      "  Total rows: 1840\n",
      "  Rows with id: 1830\n",
      "  Rows with Umsatz: 0\n",
      "  Rows with both id and Umsatz: 0\n",
      "\n",
      "2. Unique IDs Analysis:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Training Set:\n",
      "  Unique IDs: 7493\n",
      "  ID range: 1307011 to 1707315\n",
      "\n",
      "Validation Set:\n",
      "  Unique IDs: 1841\n",
      "  ID range: 1708011 to 1807315\n",
      "\n",
      "Test Set:\n",
      "  Unique IDs: 1830\n",
      "  ID range: 1808011 to 1907305\n",
      "\n",
      "3. Date Range Verification:\n",
      "------------------------------------------------------------\n",
      "Training: 2013-07-01 to 2017-07-31\n",
      "Validation: 2017-08-01 to 2018-07-31\n",
      "Test: 2018-08-01 to 2019-07-31\n",
      "\n",
      "✓ No overlap between Training and Validation\n",
      "✓ No overlap between Validation and Test\n"
     ]
    }
   ],
   "source": [
    "# Verify row counts and IDs for each split\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA INTEGRITY VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n1. Missing Values Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for split_name, split_df in [('Training', df_train), ('Validation', df_validation), ('Test', df_test)]:\n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    rows_with_id = split_df['id'].notna().sum()\n",
    "    rows_with_umsatz = split_df['Umsatz'].notna().sum()\n",
    "    print(f\"  Total rows: {len(split_df)}\")\n",
    "    print(f\"  Rows with id: {rows_with_id}\")\n",
    "    print(f\"  Rows with Umsatz: {rows_with_umsatz}\")\n",
    "    print(f\"  Rows with both id and Umsatz: {sum((split_df['id'].notna()) & (split_df['Umsatz'].notna()))}\")\n",
    "\n",
    "# Check unique IDs\n",
    "print(\"\\n2. Unique IDs Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for split_name, split_df in [('Training', df_train), ('Validation', df_validation), ('Test', df_test)]:\n",
    "    unique_ids = split_df['id'].dropna().unique()\n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    print(f\"  Unique IDs: {len(unique_ids)}\")\n",
    "    print(f\"  ID range: {split_df['id'].min():.0f} to {split_df['id'].max():.0f}\")\n",
    "\n",
    "# Check for no overlap between splits\n",
    "print(\"\\n3. Date Range Verification:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Training: {df_train['Datum'].min().date()} to {df_train['Datum'].max().date()}\")\n",
    "print(f\"Validation: {df_validation['Datum'].min().date()} to {df_validation['Datum'].max().date()}\")\n",
    "print(f\"Test: {df_test['Datum'].min().date()} to {df_test['Datum'].max().date()}\")\n",
    "\n",
    "# Verify no date overlaps\n",
    "if df_train['Datum'].max() < df_validation['Datum'].min():\n",
    "    print(\"\\n✓ No overlap between Training and Validation\")\n",
    "else:\n",
    "    print(\"\\n✗ WARNING: Overlap between Training and Validation!\")\n",
    "\n",
    "if df_validation['Datum'].max() < df_test['Datum'].min():\n",
    "    print(\"✓ No overlap between Validation and Test\")\n",
    "else:\n",
    "    print(\"✗ WARNING: Overlap between Validation and Test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39317c52",
   "metadata": {},
   "source": [
    "## Save the Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf60f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully!\n",
      "\n",
      "✓ train_set.csv (7523 rows)\n",
      "✓ validation_set.csv (1849 rows)\n",
      "✓ test_set.csv (1840 rows)\n",
      "\n",
      "Total: 11212 rows saved\n"
     ]
    }
   ],
   "source": [
    "# Define output file names with processed_data directory\n",
    "output_train = os.path.join(processed_data_dir, 'train_set.csv')\n",
    "output_val = os.path.join(processed_data_dir, 'validation_set.csv')\n",
    "output_test = os.path.join(processed_data_dir, 'test_set.csv')\n",
    "\n",
    "# Create processed_data directory if it doesn't exist\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "# Save the splits to CSV files\n",
    "df_train.to_csv(output_train, index=False)\n",
    "df_validation.to_csv(output_val, index=False)\n",
    "df_test.to_csv(output_test, index=False)\n",
    "\n",
    "print(\"Datasets saved successfully!\")\n",
    "print(f\"\\n✓ {output_train} ({len(df_train)} rows)\")\n",
    "print(f\"✓ {output_val} ({len(df_validation)} rows)\")\n",
    "print(f\"✓ {output_test} ({len(df_test)} rows)\")\n",
    "print(f\"\\nTotal: {len(df_train) + len(df_validation) + len(df_test)} rows saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7adb5a3",
   "metadata": {},
   "source": [
    "## Summary Statistics for Each Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80dee48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPLIT SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Training Set (2013-07-01 to 2017-07-31):\n",
      "--------------------------------------------------------------------------------\n",
      "Rows with sales data (Umsatz): 7493\n",
      "Rows with weather data: 7517\n",
      "Unique product groups (Warengruppe): 6\n",
      "\n",
      "Umsatz statistics:\n",
      "count    7493.000000\n",
      "mean      209.338996\n",
      "std       147.769192\n",
      "min        12.937383\n",
      "25%        96.774910\n",
      "50%       162.622977\n",
      "75%       283.910218\n",
      "max      1879.461831\n",
      "Name: Umsatz, dtype: float64\n",
      "\n",
      "Temperatur statistics:\n",
      "count    7517.000000\n",
      "mean       12.069258\n",
      "std         7.022327\n",
      "min        -8.475000\n",
      "25%         6.625000\n",
      "50%        11.500000\n",
      "75%        17.837500\n",
      "max        31.437500\n",
      "Name: Temperatur, dtype: float64\n",
      "\n",
      "Holidays in this period: 227\n",
      "\n",
      "Validation Set (2017-08-01 to 2018-07-31):\n",
      "--------------------------------------------------------------------------------\n",
      "Rows with sales data (Umsatz): 1841\n",
      "Rows with weather data: 1839\n",
      "Unique product groups (Warengruppe): 6\n",
      "\n",
      "Umsatz statistics:\n",
      "count    1841.000000\n",
      "mean      196.207759\n",
      "std       130.111002\n",
      "min         7.051201\n",
      "25%        97.906026\n",
      "50%       158.940547\n",
      "75%       262.350248\n",
      "max      1432.422347\n",
      "Name: Umsatz, dtype: float64\n",
      "\n",
      "Temperatur statistics:\n",
      "count    1839.000000\n",
      "mean       11.804421\n",
      "std         8.008686\n",
      "min        -6.137500\n",
      "25%         4.681250\n",
      "50%        12.187500\n",
      "75%        18.656250\n",
      "max        31.287500\n",
      "Name: Temperatur, dtype: float64\n",
      "\n",
      "Holidays in this period: 48\n",
      "\n",
      "Test Set (2018-08-01 to 2019-07-31):\n",
      "--------------------------------------------------------------------------------\n",
      "Rows with sales data (Umsatz): 0\n",
      "Rows with weather data: 1775\n",
      "Unique product groups (Warengruppe): 6\n",
      "\n",
      "Umsatz statistics:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: Umsatz, dtype: float64\n",
      "\n",
      "Temperatur statistics:\n",
      "count    1775.000000\n",
      "mean       11.983008\n",
      "std         7.099904\n",
      "min        -4.812500\n",
      "25%         6.500000\n",
      "50%        11.162500\n",
      "75%        17.325000\n",
      "max        32.671428\n",
      "Name: Temperatur, dtype: float64\n",
      "\n",
      "Holidays in this period: 14\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics for each split\n",
    "print(\"=\" * 80)\n",
    "print(\"SPLIT SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for split_name, split_df in [('Training', df_train), ('Validation', df_validation), ('Test', df_test)]:\n",
    "    print(f\"\\n{split_name} Set (2013-07-01 to 2017-07-31):\" if split_name == 'Training'\n",
    "          else f\"\\n{split_name} Set (2017-08-01 to 2018-07-31):\" if split_name == 'Validation'\n",
    "          else f\"\\n{split_name} Set (2018-08-01 to 2019-07-31):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Rows with sales data (Umsatz): {split_df['Umsatz'].notna().sum()}\")\n",
    "    print(f\"Rows with weather data: {split_df['Temperatur'].notna().sum()}\")\n",
    "    print(f\"Unique product groups (Warengruppe): {split_df['Warengruppe'].nunique()}\")\n",
    "    print(f\"\\nUmsatz statistics:\")\n",
    "    print(split_df['Umsatz'].describe())\n",
    "    print(f\"\\nTemperatur statistics:\")\n",
    "    print(split_df['Temperatur'].describe())\n",
    "    print(f\"\\nHolidays in this period: {split_df['is_holiday'].sum():.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
