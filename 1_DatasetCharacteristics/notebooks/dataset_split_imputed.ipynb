{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c12473",
   "metadata": {},
   "source": [
    "## Load the Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c9e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset loaded successfully\n",
      "Shape: (11211, 48)\n",
      "Date range: 2013-07-01 00:00:00 to 2019-07-30 00:00:00\n",
      "\n",
      "First few rows:\n",
      "       Datum         id  Warengruppe      Umsatz  KielerWoche  Bewoelkung  \\\n",
      "0 2013-07-01  1307011.0          1.0  148.828353          0.0         6.0   \n",
      "1 2013-07-01  1307013.0          3.0  201.198426          0.0         6.0   \n",
      "2 2013-07-01  1307014.0          4.0   65.890169          0.0         6.0   \n",
      "3 2013-07-01  1307015.0          5.0  317.475875          0.0         6.0   \n",
      "4 2013-07-01  1307012.0          2.0  535.856285          0.0         6.0   \n",
      "\n",
      "   Temperatur  Windgeschwindigkeit  Wettercode  Niederschlag  ...  W_Cat_4  \\\n",
      "0     17.8375                 15.0        20.0           0.3  ...        1   \n",
      "1     17.8375                 15.0        20.0           0.3  ...        1   \n",
      "2     17.8375                 15.0        20.0           0.3  ...        1   \n",
      "3     17.8375                 15.0        20.0           0.3  ...        1   \n",
      "4     17.8375                 15.0        20.0           0.3  ...        1   \n",
      "\n",
      "   W_Cat_5  W_Cat_6  W_Cat_7  W_Cat_8  W_Cat_9  W_Cat_10  Temperatur_kalt  \\\n",
      "0        0        0        0        0        0         0                1   \n",
      "1        0        0        0        0        0         0                1   \n",
      "2        0        0        0        0        0         0                1   \n",
      "3        0        0        0        0        0         0                1   \n",
      "4        0        0        0        0        0         0                1   \n",
      "\n",
      "   Temperatur_normal  Temperatur_warm  \n",
      "0                  0                0  \n",
      "1                  0                0  \n",
      "2                  0                0  \n",
      "3                  0                0  \n",
      "4                  0                0  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11211 entries, 0 to 11210\n",
      "Data columns (total 48 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   Datum                      11211 non-null  datetime64[ns]\n",
      " 1   id                         11164 non-null  float64       \n",
      " 2   Warengruppe                11164 non-null  float64       \n",
      " 3   Umsatz                     9334 non-null   float64       \n",
      " 4   KielerWoche                11211 non-null  float64       \n",
      " 5   Bewoelkung                 11211 non-null  float64       \n",
      " 6   Temperatur                 11211 non-null  float64       \n",
      " 7   Windgeschwindigkeit        11211 non-null  float64       \n",
      " 8   Wettercode                 11211 non-null  float64       \n",
      " 9   Niederschlag               11211 non-null  float64       \n",
      " 10  is_holiday                 11211 non-null  int64         \n",
      " 11  Temperatur_imputed         11211 non-null  int64         \n",
      " 12  Bewoelkung_imputed         11211 non-null  int64         \n",
      " 13  Windgeschw_imputed         11211 non-null  int64         \n",
      " 14  Niederschlag_imputed       11211 non-null  int64         \n",
      " 15  Wettercode_imputed         11211 non-null  int64         \n",
      " 16  Warengruppe_1.0            11211 non-null  int64         \n",
      " 17  Warengruppe_2.0            11211 non-null  int64         \n",
      " 18  Warengruppe_3.0            11211 non-null  int64         \n",
      " 19  Warengruppe_4.0            11211 non-null  int64         \n",
      " 20  Warengruppe_5.0            11211 non-null  int64         \n",
      " 21  Warengruppe_6.0            11211 non-null  int64         \n",
      " 22  Warengruppe_nan            11211 non-null  int64         \n",
      " 23  Weekday_Monday             11211 non-null  int64         \n",
      " 24  Weekday_Tuesday            11211 non-null  int64         \n",
      " 25  Weekday_Wednesday          11211 non-null  int64         \n",
      " 26  Weekday_Thursday           11211 non-null  int64         \n",
      " 27  Weekday_Friday             11211 non-null  int64         \n",
      " 28  Weekday_Saturday           11211 non-null  int64         \n",
      " 29  Weekday_Sunday             11211 non-null  int64         \n",
      " 30  Temperatur_7day_rolling    11211 non-null  float64       \n",
      " 31  Niederschlag_trocken       11211 non-null  int64         \n",
      " 32  Niederschlag_nass          11211 non-null  int64         \n",
      " 33  Niederschlag_7day_rolling  11211 non-null  float64       \n",
      " 34  W_Cat_-1                   11211 non-null  int64         \n",
      " 35  W_Cat_1                    11211 non-null  int64         \n",
      " 36  W_Cat_2                    11211 non-null  int64         \n",
      " 37  W_Cat_3                    11211 non-null  int64         \n",
      " 38  W_Cat_4                    11211 non-null  int64         \n",
      " 39  W_Cat_5                    11211 non-null  int64         \n",
      " 40  W_Cat_6                    11211 non-null  int64         \n",
      " 41  W_Cat_7                    11211 non-null  int64         \n",
      " 42  W_Cat_8                    11211 non-null  int64         \n",
      " 43  W_Cat_9                    11211 non-null  int64         \n",
      " 44  W_Cat_10                   11211 non-null  int64         \n",
      " 45  Temperatur_kalt            11211 non-null  int64         \n",
      " 46  Temperatur_normal          11211 non-null  int64         \n",
      " 47  Temperatur_warm            11211 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(11), int64(36)\n",
      "memory usage: 4.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set up path for processed data\n",
    "processed_data_dir = '../processed_data'\n",
    "\n",
    "# Load the combined dataset\n",
    "file_path = os.path.join(processed_data_dir, 'combined_data_final_imputed.csv')\n",
    "df = pd.read_csv(file_path, parse_dates=['Datum'])\n",
    "\n",
    "print(f\"Combined dataset loaded successfully\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Datum'].min()} to {df['Datum'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9cf7a3",
   "metadata": {},
   "source": [
    "## Define Date Ranges and Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba574891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits created successfully!\n",
      "\n",
      "Training set: 2013-07-01 to 2017-07-31\n",
      "  Rows: 7523\n",
      "\n",
      "Validation set: 2017-08-01 to 2018-07-31\n",
      "  Rows: 1849\n",
      "\n",
      "Test set: 2018-08-01 to 2019-07-31\n",
      "  Rows: 1839\n",
      "\n",
      "Total rows in all splits: 11211\n"
     ]
    }
   ],
   "source": [
    "# Define the date ranges for the splits\n",
    "train_start = pd.Timestamp('2013-07-01')\n",
    "train_end = pd.Timestamp('2017-07-31')\n",
    "\n",
    "val_start = pd.Timestamp('2017-08-01')\n",
    "val_end = pd.Timestamp('2018-07-31')\n",
    "\n",
    "test_start = pd.Timestamp('2018-08-01')\n",
    "test_end = pd.Timestamp('2019-07-31')\n",
    "\n",
    "# Split the dataset based on date ranges\n",
    "df_train = df[(df['Datum'] >= train_start) & (df['Datum'] <= train_end)].copy()\n",
    "df_validation = df[(df['Datum'] >= val_start) & (df['Datum'] <= val_end)].copy()\n",
    "df_test = df[(df['Datum'] >= test_start) & (df['Datum'] <= test_end)].copy()\n",
    "\n",
    "print(\"Dataset splits created successfully!\\n\")\n",
    "print(f\"Training set: {train_start.date()} to {train_end.date()}\")\n",
    "print(f\"  Rows: {len(df_train)}\")\n",
    "print(f\"\\nValidation set: {val_start.date()} to {val_end.date()}\")\n",
    "print(f\"  Rows: {len(df_validation)}\")\n",
    "print(f\"\\nTest set: {test_start.date()} to {test_end.date()}\")\n",
    "print(f\"  Rows: {len(df_test)}\")\n",
    "print(f\"\\nTotal rows in all splits: {len(df_train) + len(df_validation) + len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba1cba",
   "metadata": {},
   "source": [
    "## Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5a4d722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA INTEGRITY VERIFICATION\n",
      "============================================================\n",
      "\n",
      "1. Missing Values Analysis:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Training Set:\n",
      "  Total rows: 7523\n",
      "  Rows with id: 7493\n",
      "  Rows with Umsatz: 7493\n",
      "  Rows with both id and Umsatz: 7493\n",
      "\n",
      "Validation Set:\n",
      "  Total rows: 1849\n",
      "  Rows with id: 1841\n",
      "  Rows with Umsatz: 1841\n",
      "  Rows with both id and Umsatz: 1841\n",
      "\n",
      "Test Set:\n",
      "  Total rows: 1839\n",
      "  Rows with id: 1830\n",
      "  Rows with Umsatz: 0\n",
      "  Rows with both id and Umsatz: 0\n",
      "\n",
      "2. Unique IDs Analysis:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Training Set:\n",
      "  Unique IDs: 7493\n",
      "  ID range: 1307011 to 1707315\n",
      "\n",
      "Validation Set:\n",
      "  Unique IDs: 1841\n",
      "  ID range: 1708011 to 1807315\n",
      "\n",
      "Test Set:\n",
      "  Unique IDs: 1830\n",
      "  ID range: 1808011 to 1907305\n",
      "\n",
      "3. Date Range Verification:\n",
      "------------------------------------------------------------\n",
      "Training: 2013-07-01 to 2017-07-31\n",
      "Validation: 2017-08-01 to 2018-07-31\n",
      "Test: 2018-08-01 to 2019-07-30\n",
      "\n",
      "✓ No overlap between Training and Validation\n",
      "✓ No overlap between Validation and Test\n"
     ]
    }
   ],
   "source": [
    "# Verify row counts and IDs for each split\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA INTEGRITY VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n1. Missing Values Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for split_name, split_df in [('Training', df_train), ('Validation', df_validation), ('Test', df_test)]:\n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    rows_with_id = split_df['id'].notna().sum()\n",
    "    rows_with_umsatz = split_df['Umsatz'].notna().sum()\n",
    "    print(f\"  Total rows: {len(split_df)}\")\n",
    "    print(f\"  Rows with id: {rows_with_id}\")\n",
    "    print(f\"  Rows with Umsatz: {rows_with_umsatz}\")\n",
    "    print(f\"  Rows with both id and Umsatz: {sum((split_df['id'].notna()) & (split_df['Umsatz'].notna()))}\")\n",
    "\n",
    "# Check unique IDs\n",
    "print(\"\\n2. Unique IDs Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for split_name, split_df in [('Training', df_train), ('Validation', df_validation), ('Test', df_test)]:\n",
    "    unique_ids = split_df['id'].dropna().unique()\n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    print(f\"  Unique IDs: {len(unique_ids)}\")\n",
    "    print(f\"  ID range: {split_df['id'].min():.0f} to {split_df['id'].max():.0f}\")\n",
    "\n",
    "# Check for no overlap between splits\n",
    "print(\"\\n3. Date Range Verification:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Training: {df_train['Datum'].min().date()} to {df_train['Datum'].max().date()}\")\n",
    "print(f\"Validation: {df_validation['Datum'].min().date()} to {df_validation['Datum'].max().date()}\")\n",
    "print(f\"Test: {df_test['Datum'].min().date()} to {df_test['Datum'].max().date()}\")\n",
    "\n",
    "# Verify no date overlaps\n",
    "if df_train['Datum'].max() < df_validation['Datum'].min():\n",
    "    print(\"\\n✓ No overlap between Training and Validation\")\n",
    "else:\n",
    "    print(\"\\n✗ WARNING: Overlap between Training and Validation!\")\n",
    "\n",
    "if df_validation['Datum'].max() < df_test['Datum'].min():\n",
    "    print(\"✓ No overlap between Validation and Test\")\n",
    "else:\n",
    "    print(\"✗ WARNING: Overlap between Validation and Test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the lines without 'id' or 'Umsatz' for saving the splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39317c52",
   "metadata": {},
   "source": [
    "## Save the Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1caf60f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved successfully!\n",
      "\n",
      "✓ ../processed_data/train_set_imputed.csv (7523 rows)\n",
      "✓ ../processed_data/validation_set_imputed.csv (1849 rows)\n",
      "✓ ../processed_data/test_set_imputed.csv (1839 rows)\n",
      "\n",
      "Total: 11211 rows saved\n"
     ]
    }
   ],
   "source": [
    "# Define output file names with processed_data directory\n",
    "output_train = os.path.join(processed_data_dir, 'train_set_imputed.csv')\n",
    "output_val = os.path.join(processed_data_dir, 'validation_set_imputed.csv')\n",
    "output_test = os.path.join(processed_data_dir, 'test_set_imputed.csv')\n",
    "\n",
    "# Create processed_data directory if it doesn't exist\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "# Save the splits to CSV files\n",
    "df_train.to_csv(output_train, index=False)\n",
    "df_validation.to_csv(output_val, index=False)\n",
    "df_test.to_csv(output_test, index=False)\n",
    "\n",
    "print(\"Datasets saved successfully!\")\n",
    "print(f\"\\n✓ {output_train} ({len(df_train)} rows)\")\n",
    "print(f\"✓ {output_val} ({len(df_validation)} rows)\")\n",
    "print(f\"✓ {output_test} ({len(df_test)} rows)\")\n",
    "print(f\"\\nTotal: {len(df_train) + len(df_validation) + len(df_test)} rows saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7adb5a3",
   "metadata": {},
   "source": [
    "## Summary Statistics for Each Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80dee48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPLIT SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Training Set (2013-07-01 to 2017-07-31):\n",
      "--------------------------------------------------------------------------------\n",
      "Rows with sales data (Umsatz): 7493\n",
      "Rows with weather data: 7523\n",
      "Unique product groups (Warengruppe): 6\n",
      "\n",
      "Umsatz statistics:\n",
      "count    7493.000000\n",
      "mean      209.338996\n",
      "std       147.769192\n",
      "min        12.937383\n",
      "25%        96.774910\n",
      "50%       162.622977\n",
      "75%       283.910218\n",
      "max      1879.461831\n",
      "Name: Umsatz, dtype: float64\n",
      "\n",
      "Temperatur statistics:\n",
      "count    7523.000000\n",
      "mean       12.067209\n",
      "std         7.019900\n",
      "min        -8.475000\n",
      "25%         6.625000\n",
      "50%        11.500000\n",
      "75%        17.775000\n",
      "max        31.437500\n",
      "Name: Temperatur, dtype: float64\n",
      "\n",
      "Holidays in this period: 205\n",
      "\n",
      "Validation Set (2017-08-01 to 2018-07-31):\n",
      "--------------------------------------------------------------------------------\n",
      "Rows with sales data (Umsatz): 1841\n",
      "Rows with weather data: 1849\n",
      "Unique product groups (Warengruppe): 6\n",
      "\n",
      "Umsatz statistics:\n",
      "count    1841.000000\n",
      "mean      196.207759\n",
      "std       130.111002\n",
      "min         7.051201\n",
      "25%        97.906026\n",
      "50%       158.940547\n",
      "75%       262.350248\n",
      "max      1432.422347\n",
      "Name: Umsatz, dtype: float64\n",
      "\n",
      "Temperatur statistics:\n",
      "count    1849.000000\n",
      "mean       11.817715\n",
      "std         7.989024\n",
      "min        -6.137500\n",
      "25%         4.700000\n",
      "50%        12.225000\n",
      "75%        18.637500\n",
      "max        31.287500\n",
      "Name: Temperatur, dtype: float64\n",
      "\n",
      "Holidays in this period: 42\n",
      "\n",
      "Test Set (2018-08-01 to 2019-07-31):\n",
      "--------------------------------------------------------------------------------\n",
      "Rows with sales data (Umsatz): 0\n",
      "Rows with weather data: 1839\n",
      "Unique product groups (Warengruppe): 6\n",
      "\n",
      "Umsatz statistics:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: Umsatz, dtype: float64\n",
      "\n",
      "Temperatur statistics:\n",
      "count    1839.000000\n",
      "mean       12.131737\n",
      "std         7.048954\n",
      "min        -4.812500\n",
      "25%         6.675000\n",
      "50%        11.425000\n",
      "75%        17.337500\n",
      "max        32.671428\n",
      "Name: Temperatur, dtype: float64\n",
      "\n",
      "Holidays in this period: 46\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics for each split\n",
    "print(\"=\" * 80)\n",
    "print(\"SPLIT SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for split_name, split_df in [('Training', df_train), ('Validation', df_validation), ('Test', df_test)]:\n",
    "    print(f\"\\n{split_name} Set (2013-07-01 to 2017-07-31):\" if split_name == 'Training'\n",
    "          else f\"\\n{split_name} Set (2017-08-01 to 2018-07-31):\" if split_name == 'Validation'\n",
    "          else f\"\\n{split_name} Set (2018-08-01 to 2019-07-31):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Rows with sales data (Umsatz): {split_df['Umsatz'].notna().sum()}\")\n",
    "    print(f\"Rows with weather data: {split_df['Temperatur'].notna().sum()}\")\n",
    "    print(f\"Unique product groups (Warengruppe): {split_df['Warengruppe'].nunique()}\")\n",
    "    print(f\"\\nUmsatz statistics:\")\n",
    "    print(split_df['Umsatz'].describe())\n",
    "    print(f\"\\nTemperatur statistics:\")\n",
    "    print(split_df['Temperatur'].describe())\n",
    "    print(f\"\\nHolidays in this period: {split_df['is_holiday'].sum():.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
